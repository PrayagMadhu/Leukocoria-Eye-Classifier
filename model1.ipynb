{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensornets as nets\n",
    "import cv2\n",
    "from PIL import Image, ImageOps\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    _max=np.max(x)\n",
    "    _min=np.min(x)\n",
    "    return (x-_min)/(_max-_min)\n",
    "\n",
    "def load_img(path):\n",
    "    image_list=[]\n",
    "    for filename in glob.glob(path): \n",
    "        im=Image.open(filename)\n",
    "        image_list.append(normalize(np.array(ImageOps.fit(im ,(224,224), Image.ANTIALIAS))))\n",
    "\n",
    "    return image_list\n",
    "\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1=load_img('Traindata/leukocoria/*.jpg')\n",
    "batch_2=load_img('Traindata/non-leukocoria/*.jpg')\n",
    "test_batch1=load_img('testdata/leuko/*.jpg')\n",
    "test_batch2=load_img('testdata/non-leuko/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(dtype=tf.float32, shape=[None, 224,224, 3], name='features')\n",
    "y=tf.placeholder(dtype=tf.float32, shape=[None,2], name='labels')\n",
    "is_train=tf.placeholder_with_default(False, shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "batch_size=80\n",
    "display_step=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logits=nets.DenseNet169(x, is_training=is_train, classes=2)\n",
    "train_weights=logits.get_weights()\n",
    "loss=tf.losses.softmax_cross_entropy(y, logits)\n",
    "#train=tf.train.AdamOptimizer(1e-3).minimize(loss) \n",
    "update_ops=tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):               \n",
    "    train=tf.train.AdamOptimizer(1e-5).minimize(loss, var_list=train_weights[600:]) \n",
    "correctPred=tf.equal(tf.argmax(logits,1), tf.argmax(y,1))\n",
    "acc=tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "local_init_op=tf.local_variables_initializer()\n",
    "\n",
    "saver=tf.train.Saver()\n",
    "save_file='./logs.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "846"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope: densenet169\n",
      "Total layers: 169\n",
      "Total weights: 1142\n",
      "Total parameters: 22,328,646\n"
     ]
    }
   ],
   "source": [
    "logits.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(batch_size, batch_1, batch_2):\n",
    "    out_batches=[]\n",
    "    for start in range(0, len(batch_1), batch_size):\n",
    "        end=start+batch_size//2\n",
    "        features=batch_1[start:end]+batch_2[start:end]\n",
    "        labels=[[0.,1.]*(batch_size//2)]+[[1.,0.,]*(batch_size//2)]\n",
    "        batches=[np.array(features), np.reshape(labels, (batch_size,2))]\n",
    "        out_batches.append(batches)\n",
    "    return out_batches\n",
    "\n",
    "valid_feat=batch_2[-65:]+batch_1[-65:]\n",
    "valid_features_=np.array(valid_feat)\n",
    "valid_lab=[[1.,0.]*(65)]+[[0.,1.]*(65)]\n",
    "valid_labels_=np.reshape(valid_lab,(130,2))\n",
    "\n",
    "test_feat=test_batch2+test_batch1\n",
    "test_features_=np.array(test_feat)\n",
    "test_lab=[[1.,0.]*(30)]+[[0.,1.]*(30)]\n",
    "test_labels_=np.reshape(test_lab,(60,2))\n",
    "\n",
    "def epoch_stats(sess, epoch, last_feat, last_lab, valid_features,valid_labels):\n",
    "    cur_cost=sess.run(loss, feed_dict={x:last_feat, y:last_lab, is_train:True})\n",
    "    cur_Acc=sess.run(acc, feed_dict={x:valid_features, y:valid_labels, is_train:True})\n",
    "    print('epoch : {:<4} - cost : {:<8.3} - acc: {:<5.3}'.format(epoch, cur_cost, cur_Acc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0    - cost : 0.811    - acc: 0.692\n",
      "epoch : 1    - cost : 0.802    - acc: 0.785\n",
      "epoch : 2    - cost : 0.789    - acc: 0.815\n",
      "epoch : 3    - cost : 0.771    - acc: 0.869\n",
      "epoch : 4    - cost : 0.748    - acc: 0.915\n",
      "epoch : 5    - cost : 0.722    - acc: 0.915\n",
      "epoch : 6    - cost : 0.694    - acc: 0.923\n",
      "epoch : 7    - cost : 0.665    - acc: 0.946\n",
      "epoch : 8    - cost : 0.638    - acc: 0.954\n",
      "epoch : 9    - cost : 0.612    - acc: 0.954\n",
      "epoch : 10   - cost : 0.585    - acc: 0.946\n",
      "epoch : 11   - cost : 0.56     - acc: 0.946\n",
      "epoch : 12   - cost : 0.538    - acc: 0.946\n",
      "epoch : 13   - cost : 0.519    - acc: 0.954\n",
      "epoch : 14   - cost : 0.505    - acc: 0.954\n",
      "epoch : 15   - cost : 0.494    - acc: 0.954\n",
      "epoch : 16   - cost : 0.485    - acc: 0.954\n",
      "epoch : 17   - cost : 0.478    - acc: 0.946\n",
      "epoch : 18   - cost : 0.472    - acc: 0.946\n",
      "epoch : 19   - cost : 0.468    - acc: 0.938\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(local_init_op)\n",
    "    sess.run(logits.pretrained())\n",
    "    for e in range(epochs):\n",
    "        for train_feat, train_lab in get_batches(batch_size, batch_1, batch_2):\n",
    "            ind_list = [i for i in range(len(train_feat))]\n",
    "            shuffle(ind_list)\n",
    "            train_new  = train_feat[ind_list, :,:,:]\n",
    "            target_new = train_lab[ind_list,]\n",
    "            sess.run(train, feed_dict={x:train_new, y:target_new, is_train:True})\n",
    "        if(e%display_step==0):\n",
    "            epoch_stats(sess, e, train_new, target_new, valid_features_,valid_labels_)\n",
    "            \n",
    "            \n",
    "    model_acc=sess.run(acc, feed_dict={x:test_features_, y:test_labels_, is_train:True})\n",
    "    print(model_acc)        \n",
    "    saver.save(sess,save_file)           \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate=load_img('Evaluation data/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1=batch_1\n",
    "samp2=batch_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./logs.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    loader = tf.train.import_meta_graph('./logs.ckpt.meta')\n",
    "    loader.restore(sess, save_file) \n",
    "    predict1=[]\n",
    "    #predict2=[]\n",
    "    for start in range(0,len(evaluate), 100):\n",
    "        end=start+100\n",
    "        predict1.extend(sess.run(tf.argmax(logits,1), feed_dict={x:evaluate[start:end]}))\n",
    "        #predict2.extend(sess.run(tf.argmax(logits,1), feed_dict={x:samp2[start:end]}))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 170, 1: 30})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(predict1))\n",
    "#print(Counter(predict2))\n",
    "#print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_data=[{'id':i,'category':c} for i,c in enumerate(predict1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "header=['id', 'category']\n",
    "csv_file = \"submission10.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        for data in export_data:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
